# Complete Data Flow: From CSV Files to Production Database

## ğŸ¯ Quick Answer

The `poc_etl.raw_*` tables are populated by **CSV ingestion scripts** that load data from the file system. Here's the complete flow:

```
CSV Files (Legacy LION System)
         â†“
   [CSV INGEST SCRIPTS]
         â†“
poc_etl schema (raw_* tables) â† YOU ARE HERE
         â†“
   [ETL PIPELINE - INGEST PHASE]
         â†“
etl schema (raw_* tables)
         â†“
   [ETL PIPELINE - TRANSFORM PHASE]
         â†“
etl schema (stg_* tables)
         â†“
   [ETL PIPELINE - EXPORT PHASE]
         â†“
dbo schema (production tables)
```

---

## ğŸ“Š Complete Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         STEP 0: SOURCE DATA                          â”‚
â”‚                    (Legacy LION System Exports)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                 CSV Files in ZIP Archive or Directory:
                    - CertificateInfo.csv (1.5M rows)
                    - premiums.csv (138K rows)
                    - perf.csv (Schedule rates - 1.1M rows)
                    - perf-group.csv (33K rows)
                    - IndividualRosterExtract.csv (Brokers)
                    - OrganizationRosterExtract.csv (Brokers)
                    - BrokerLicenseExtract.csv
                    - BrokerEO.csv
                    - Fees.csv
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           STEP 1: CSV INGESTION (File System â†’ Database)             â”‚
â”‚                                                                       â”‚
â”‚  Scripts:                                                            â”‚
â”‚    â€¢ scripts/ingest-raw-data.ts (ZIP-based, full-featured)          â”‚
â”‚    â€¢ scripts/load-csv.ts (Direct directory loader)                  â”‚
â”‚                                                                       â”‚
â”‚  What happens:                                                       â”‚
â”‚    1. Extract ZIP file (or read CSV directory)                      â”‚
â”‚    2. Match CSV files to target tables by prefix                    â”‚
â”‚    3. Validate CSV column headers                                   â”‚
â”‚    4. Create schema (poc_etl, poc_raw_data, or raw_data1-N)        â”‚
â”‚    5. Create raw_* tables (all NVARCHAR columns)                    â”‚
â”‚    6. Bulk insert data (1000-5000 rows per batch)                   â”‚
â”‚    7. Verify row counts                                             â”‚
â”‚                                                                       â”‚
â”‚  Commands:                                                           â”‚
â”‚    npx tsx scripts/ingest-raw-data.ts                              â”‚
â”‚    npx tsx scripts/load-csv.ts                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 RESULT: poc_etl.raw_* Tables Populated               â”‚
â”‚                                                                       â”‚
â”‚  Tables created:                                                     â”‚
â”‚    [poc_etl].[raw_certificate_info]    - 1.5M rows                  â”‚
â”‚    [poc_etl].[raw_schedule_rates]      - 1.1M rows                  â”‚
â”‚    [poc_etl].[raw_perf_groups]         - 33K rows                   â”‚
â”‚    [poc_etl].[raw_premiums]            - 138K rows                  â”‚
â”‚    [poc_etl].[raw_individual_brokers]  - ~12K rows                  â”‚
â”‚    [poc_etl].[raw_org_brokers]         - ~500 rows                  â”‚
â”‚    [poc_etl].[raw_broker_licenses]     - Variable                   â”‚
â”‚    [poc_etl].[raw_broker_eo]           - Variable                   â”‚
â”‚    [poc_etl].[raw_fees]                - Variable                   â”‚
â”‚    [poc_etl].[raw_commissions_detail]  - Variable                   â”‚
â”‚                                                                       â”‚
â”‚  âœ… DATA IS NOW IN SQL SERVER                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       STEP 2: ETL PIPELINE - INGEST PHASE (Schema Copy)              â”‚
â”‚                                                                       â”‚
â”‚  Script: sql/ingest/copy-from-poc-etl.sql                           â”‚
â”‚                                                                       â”‚
â”‚  What happens:                                                       â”‚
â”‚    Copies ALL raw_* tables from [poc_etl] to [etl] schema          â”‚
â”‚    (This allows ETL to work in isolated [etl] workspace)            â”‚
â”‚                                                                       â”‚
â”‚  Example SQL:                                                        â”‚
â”‚    INSERT INTO [etl].[raw_certificate_info]                         â”‚
â”‚    SELECT * FROM [poc_etl].[raw_certificate_info];                  â”‚
â”‚                                                                       â”‚
â”‚  Command:                                                            â”‚
â”‚    npx tsx scripts/run-pipeline.ts                                  â”‚
â”‚    (Includes ingest phase by default)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               RESULT: etl.raw_* Tables Populated                     â”‚
â”‚                                                                       â”‚
â”‚  Tables:                                                             â”‚
â”‚    [etl].[raw_certificate_info]    - Copy of poc_etl data           â”‚
â”‚    [etl].[raw_schedule_rates]      - Copy of poc_etl data           â”‚
â”‚    [etl].[raw_perf_groups]         - Copy of poc_etl data           â”‚
â”‚    [etl].[raw_premiums]            - Copy of poc_etl data           â”‚
â”‚    ... (all raw_ tables copied)                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      STEP 3: ETL PIPELINE - INGEST PHASE (Input Table Population)   â”‚
â”‚                                                                       â”‚
â”‚  Script: sql/ingest/populate-input-tables.sql                       â”‚
â”‚                                                                       â”‚
â”‚  What happens:                                                       â”‚
â”‚    Populates input_* tables from raw_* tables                       â”‚
â”‚    (Cleans data, normalizes formats)                                â”‚
â”‚                                                                       â”‚
â”‚  Example:                                                            â”‚
â”‚    INSERT INTO [etl].[input_certificate_info]                       â”‚
â”‚    SELECT <cleaned columns>                                         â”‚
â”‚    FROM [etl].[raw_certificate_info];                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        STEP 4: ETL PIPELINE - TRANSFORM PHASE (40+ Scripts)          â”‚
â”‚                                                                       â”‚
â”‚  Scripts: sql/transforms/00-references.sql through                  â”‚
â”‚           sql/transforms/99-audit-and-cleanup.sql                   â”‚
â”‚                                                                       â”‚
â”‚  What happens:                                                       â”‚
â”‚    - Build reference tables (States, Products)                      â”‚
â”‚    - Transform brokers (Individual + Org merge)                     â”‚
â”‚    - Transform groups (with PrimaryBrokerID)                        â”‚
â”‚    - Create schedules (from raw_schedule_rates)                     â”‚
â”‚    - Build proposals (multi-tiered)                                 â”‚
â”‚    - Create hierarchies (with ScheduleId linking)                   â”‚
â”‚    - Transform policies/certificates                                â”‚
â”‚    - Create premium transactions                                    â”‚
â”‚    - Build policy-hierarchy assignments                             â”‚
â”‚                                                                       â”‚
â”‚  Result: etl.stg_* tables (staging tables ready for export)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           STEP 5: ETL PIPELINE - EXPORT PHASE                        â”‚
â”‚                                                                       â”‚
â”‚  Scripts: sql/export/01-export-brokers.sql through                  â”‚
â”‚           sql/export/12-export-policy-hierarchy-assignments.sql     â”‚
â”‚                                                                       â”‚
â”‚  What happens:                                                       â”‚
â”‚    Copies staging tables (etl.stg_*) to production (dbo.*)         â”‚
â”‚                                                                       â”‚
â”‚  Example:                                                            â”‚
â”‚    INSERT INTO [dbo].[Brokers] (Id, Name, Status, ...)             â”‚
â”‚    SELECT Id, Name, Status, ...                                     â”‚
â”‚    FROM [etl].[stg_brokers]                                         â”‚
â”‚    WHERE NOT EXISTS (SELECT 1 FROM [dbo].[Brokers] ...);           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FINAL: Production Database                        â”‚
â”‚                                                                       â”‚
â”‚  Tables:                                                             â”‚
â”‚    [dbo].[Brokers]                                                  â”‚
â”‚    [dbo].[EmployerGroups]                                           â”‚
â”‚    [dbo].[Products]                                                 â”‚
â”‚    [dbo].[Schedules]                                                â”‚
â”‚    [dbo].[ScheduleRates]                                            â”‚
â”‚    [dbo].[Proposals]                                                â”‚
â”‚    [dbo].[Hierarchies]                                              â”‚
â”‚    [dbo].[HierarchyParticipants]                                    â”‚
â”‚    [dbo].[Policies]                                                 â”‚
â”‚    [dbo].[PremiumTransactions]                                      â”‚
â”‚    [dbo].[PolicyHierarchyAssignments]                               â”‚
â”‚    ... (and more)                                                   â”‚
â”‚                                                                       â”‚
â”‚  âœ… READY FOR COMMISSION CALCULATIONS                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” Detailed Explanation: How poc_etl.raw_* Tables Get Populated

### Option 1: Advanced ZIP-Based Ingest (Recommended)

**Script:** `scripts/ingest-raw-data.ts`

**Features:**
- Auto-detects ZIP files in `~/Downloads`
- Extracts CSV files from ZIP
- Validates column headers against expected schema
- Auto-selects next available schema (`raw_data1`, `raw_data2`, etc.)
- Preview mode (10 records per table)
- Dry-run mode (show what would happen)

**Usage:**
```bash
# Auto-detect ZIP and load into poc_etl
npx tsx scripts/ingest-raw-data.ts --schema poc_etl

# With specific ZIP file
npx tsx scripts/ingest-raw-data.ts \
  --zip ~/Downloads/data-2026-01-19.zip \
  --schema poc_etl

# Preview mode (10 records per table)
npx tsx scripts/ingest-raw-data.ts --schema poc_etl --preview

# Dry run (see what would happen)
npx tsx scripts/ingest-raw-data.ts --schema poc_etl --dry-run
```

**What it does:**
1. Finds ZIP file (or uses specified path)
2. Extracts all CSV files to temp directory
3. Matches CSV files to target tables by prefix:
   - `CertificateInfo*.csv` â†’ `raw_certificate_info`
   - `premiums*.csv` â†’ `raw_premiums`
   - `APL-Perf_Schedule*.csv` â†’ `raw_schedule_rates`
   - `IndividualRosterExtract*.csv` â†’ `raw_individual_brokers`
   - etc.
4. Creates `poc_etl` schema (if doesn't exist)
5. Creates all `raw_*` tables (with all NVARCHAR columns)
6. Bulk inserts data (1000 rows per batch)
7. Verifies row counts

---

### Option 2: Direct CSV Loader

**Script:** `scripts/load-csv.ts`

**Usage:**
```bash
# Load from hardcoded CSV directory
npx tsx scripts/load-csv.ts

# Test with 100 rows per file
npx tsx scripts/load-csv.ts --limit 100
```

**What it does:**
1. Reads CSV files from configured directory
2. Dynamically detects column names from CSV headers
3. Creates tables with all NVARCHAR(MAX) columns
4. Uses SQL Server `BULK INSERT` (5000 rows per batch - very fast)
5. Handles patterns like `CommissionsDetail_*.csv` (multiple files)

**CSV Directory:**
```typescript
// Hardcoded in script (line 37)
const csvDataPath = '/Users/kennpalm/Downloads/source/APL/apl-commissions-frontend/docs/data-map/rawdata';
```

---

### Option 3: POC Schema Setup (For Testing)

**Script:** `scripts/setup-poc-schemas.ts`

**Usage:**
```bash
npx tsx scripts/setup-poc-schemas.ts
```

**What it does:**
1. Creates `poc_etl`, `poc_dbo`, `poc_raw_data` schemas
2. Copies **sample data** from existing `etl` schema (if it exists)
3. Creates only 100 records per table (for testing)
4. Sets up state management tables

**Use case:** When you already have data in `etl` and want a small POC copy

---

## ğŸ“‹ File-to-Table Mapping

| CSV File Pattern | Target Table | Key Fields |
|------------------|--------------|------------|
| `CertificateInfo*.csv` | `raw_certificate_info` | CertificateId, GroupId, Product |
| `premiums*.csv` | `raw_premiums` | Policy, GroupNumber, Amount |
| `APL-Perf_Schedule*.csv` | `raw_schedule_rates` | ScheduleName, ProductCode, State |
| `APL-Perf_Group*.csv` | `raw_perf_groups` | GroupId, GroupName, Size |
| `IndividualRosterExtract*.csv` | `raw_individual_brokers` | PartyUniqueId, Name, Status |
| `OrganizationRosterExtract*.csv` | `raw_org_brokers` | PartyUniqueId, OrgName |
| `BrokerLicenseExtract*.csv` | `raw_broker_licenses` | PartyUniqueId, State |
| `BrokerEO*.csv` | `raw_broker_eo` | PartyUniqueId, PolicyId |
| `CommissionsDetail*.csv` | `raw_commissions_detail` | CertificateId, BrokerId |
| `Fees*.csv` | `raw_fees` | Various |

---

## ğŸš€ Complete Workflow Example

### Step 1: Get CSV Data from Client
```bash
# Receive ZIP file from client
# File: APL-Data-Export-2026-01-19.zip
# Location: ~/Downloads/
```

### Step 2: Ingest CSV â†’ poc_etl
```bash
cd /Users/kennpalm/Downloads/source/APL/apl-commissions-etl

# Test with preview (10 records per table)
npx tsx scripts/ingest-raw-data.ts --schema poc_etl --preview

# Verify preview data looks good
# Then do full load
npx tsx scripts/ingest-raw-data.ts --schema poc_etl
```

**Result:** `poc_etl.raw_*` tables populated with ~2.8M rows

### Step 3: Run ETL Pipeline
```bash
# Full pipeline (includes copy from poc_etl â†’ etl)
npx tsx scripts/run-pipeline.ts

# Or with step-by-step verification
npx tsx scripts/run-pipeline.ts --step-by-step
```

**Pipeline phases:**
1. âœ… Schema Setup (creates/resets etl schema)
2. âœ… **Data Ingest** (copies poc_etl â†’ etl, populates input tables)
3. âœ… Transforms (40+ SQL scripts)
4. âœ… Export (etl.stg_* â†’ dbo.*)

### Step 4: Verify Production Data
```sql
-- Check production table counts
SELECT 'Brokers' as tbl, COUNT(*) as cnt FROM [dbo].[Brokers]
UNION ALL SELECT 'EmployerGroups', COUNT(*) FROM [dbo].[EmployerGroups]
UNION ALL SELECT 'Policies', COUNT(*) FROM [dbo].[Policies]
UNION ALL SELECT 'Hierarchies', COUNT(*) FROM [dbo].[Hierarchies]
UNION ALL SELECT 'Proposals', COUNT(*) FROM [dbo].[Proposals];
```

---

## ğŸ¯ Summary

**Your Question:** "Where does the data come from? How are `poc_etl.raw_*` tables populated?"

**Answer:**

1. **Original Source:** CSV files exported from legacy LION system
2. **First Step:** CSV ingest scripts (`ingest-raw-data.ts` or `load-csv.ts`) read CSV files from disk
3. **Result:** Create `poc_etl` schema and populate `raw_*` tables via bulk insert
4. **ETL Pipeline:** Copies from `poc_etl` to `etl` schema, then transforms to `stg_*`, then exports to `dbo.*`

**The `poc_etl.raw_*` tables are populated by running CSV ingestion scripts, NOT by the main ETL pipeline. The ETL pipeline starts AFTER those tables are already populated.**

---

## ğŸ“š Additional Documentation

- **CSV Ingest Features:** `INGEST_IMPROVEMENTS.md`
- **Pipeline Flow:** `PIPELINE-FLOW-DIAGRAM.md`
- **Pipeline Updates:** `PIPELINE-UPDATES-INGEST-PHASE.md`
- **Step-by-Step Guide:** `INGEST-STEP-BY-STEP-GUIDE.md`
